<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Security Hub - The Future of Digital Defense</title>
    <link rel="stylesheet" type="text/css" href="styles2.css">
</head>
<body>
    <div class="bg-animation"></div>
    
    <nav class="navbar" id="navbar">
        <div class="nav-container">
            <a href="#" class="logo">AI Security Hub</a>
            <ul class="nav-menu">
                <li><a href="index.html">Home</a></li>
                
            </ul>
        </div>
    </nav>

    <section class="hero" id="home">
        <div class="hero-content">
            <h1 class="hero-title">AI Security</h1>
            <p class="hero-subtitle">The Future of Digital Defense</p>
            <p class="hero-description">
                Navigate the evolving landscape of AI-powered cybersecurity threats and defenses. 
                Stay ahead of adversarial attacks, secure your ML models, and build resilient AI systems.
            </p>
            <a href="https://www.sophos.com/en-us/solutions/ai-cybersecurity/ai-cybersecurity-toolkit" class="cta-button">Explore Threats</a>
        </div>
        
        <div class="floating-element">
            <div class="feature-icon">üõ°Ô∏è</div>
            <div>Neural Defense</div>
        </div>
        
        <div class="floating-element">
            <div class="feature-icon">‚ö°</div>
            <div>Real-time Detection</div>
        </div>
        
        <div class="floating-element">
            <div class="feature-icon">üîÆ</div>
            <div>Predictive Security</div>
        </div>
    </section>

    <section class="features fade-in" id="features">
        <div class="container">
            <h2 class="section-title">AI Threat Landscape</h2>
            
            <div class="features-grid">
                <div class="feature-card">
                    <span class="feature-icon">üéØ</span>
                    <h3 class="feature-title">Adversarial Attacks</h3>
                    <p class="feature-description">
                        Sophisticated attacks that manipulate AI models through carefully crafted inputs, 
                        causing misclassification and system failures.
                    </p>
                </div>
                
                <div class="feature-card">
                    <span class="feature-icon">üß¨</span>
                    <h3 class="feature-title">Data Poisoning</h3>
                    <p class="feature-description">
                        Malicious actors inject corrupted data into training sets to compromise model 
                        integrity and create backdoors for future exploitation.
                    </p>
                </div>
                
                <div class="feature-card">
                    <span class="feature-icon">üîç</span>
                    <h3 class="feature-title">Model Extraction</h3>
                    <p class="feature-description">
                        Reverse engineering techniques used to steal proprietary AI models through 
                        strategic queries and response analysis.
                    </p>
                </div>
                
                <div class="feature-card">
                    <span class="feature-icon">üé≠</span>
                    <h3 class="feature-title">Privacy Attacks</h3>
                    <p class="feature-description">
                        Membership inference and model inversion attacks that extract sensitive 
                        training data and violate user privacy.
                    </p>
                </div>
                
                <div class="feature-card">
                    <span class="feature-icon">üåä</span>
                    <h3 class="feature-title">Prompt Injection</h3>
                    <p class="feature-description">
                        Malicious prompts designed to bypass safety measures and manipulate 
                        language models into producing harmful or unintended outputs.
                    </p>
                </div>
                
                <div class="feature-card">
                    <span class="feature-icon">‚öôÔ∏è</span>
                    <h3 class="feature-title">Model Hijacking</h3>
                    <p class="feature-description">
                        Advanced techniques to take control of deployed AI systems and redirect 
                        them for unauthorized purposes or data exfiltration.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="threat-matrix fade-in" id="matrix">
        <div class="container">
            <h2 class="section-title">Threat Severity Matrix</h2>
            
            <div class="matrix-grid">
                <div class="threat-item">
                    <div class="threat-level high">Critical</div>
                    <h4>Zero-Day AI Exploits</h4>
                </div>
                
                <div class="threat-item">
                    <div class="threat-level high">Critical</div>
                    <h4>Deepfake Weaponization</h4>
                </div>
                
                <div class="threat-item">
                    <div class="threat-level medium">High</div>
                    <h4>Automated Social Engineering</h4>
                </div>
                
                <div class="threat-item">
                    <div class="threat-level medium">High</div>
                    <h4>AI-Generated Malware</h4>
                </div>
                
                <div class="threat-item">
                    <div class="threat-level low">Medium</div>
                    <h4>Biometric Spoofing</h4>
                </div>
                
                <div class="threat-item">
                    <div class="threat-level low">Medium</div>
                    <h4>Algorithm Bias Exploitation</h4>
                </div>
            </div>
        </div>
    </section>

    <section class="stats-section fade-in" id="stats">
        <div class="container">
            <h2 class="section-title">AI Security Statistics</h2>
            
            <div class="stats-grid">
                <div class="stat-card">
                    <span class="stat-number">87%</span>
                    <span class="stat-label">AI Models Vulnerable</span>
                </div>
                
                <div class="stat-card">
                    <span class="stat-number">340%</span>
                    <span class="stat-label">Attack Growth Rate</span>
                </div>
                
                <div class="stat-card">
                    <span class="stat-number">15.2s</span>
                    <span class="stat-label">Average Breach Time</span>
                </div>
                
                <div class="stat-card">
                    <span class="stat-number">$4.8M</span>
                    <span class="stat-label">Cost Per AI Breach</span>
                </div>
            </div>
        </div>
    </section>

    

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
              <div class="footer-content">
                <div class="footer-section">
                    <h3>Quick Links</h3>
                    <a href="index.html">Home</a>
                    <a href="about.html">About</a>
                    
                </div>
                <div class="footer-section">
                    <h3>Threats</h3>
                    <a href="https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf">Adversarial AI</a>
                    <a href="https://arxiv.org/abs/2004.06315">Data Poisoning</a>
                    <a href="https://arxiv.org/abs/1806.04049">Model Extraction</a>
                    <a href="https://arxiv.org/abs/2007.07646">Privacy Attacks</a>
                </div>
                
                <div class="footer-section">
                    <h3>Defense</h3>
                    <a href="https://arxiv.org/abs/2103.05920">Secure ML</a>
                    <a href="https://arxiv.org/abs/1905.05130">Model Hardening</a>
                    <a href="https://arxiv.org/abs/2007.07274">Threat Detection</a>
                    <a href="https://arxiv.org/abs/2202.05634">Incident Response</a>
                </div>
                
                
            </div>
            
            <div class="footer-bottom">
                <p>&copy; 2025 CyberAI Insights. Exploring the intersection of AI and cybersecurity</p>
            </div>
        </div>
    </footer>
    
    <script src="script2.js"></script>

    
